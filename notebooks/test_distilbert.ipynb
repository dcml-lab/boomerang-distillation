{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "WKG-T8E_8sQz",
   "metadata": {
    "id": "WKG-T8E_8sQz"
   },
   "source": [
    "# DistilBERT Boomerang Distillation Interpolation\n",
    "\n",
    "\n",
    "*   We evaluate interpolation between DistilBERT and BERT on pseudo-perplexity for a subset of 50 Wikitext documents (note: we use full Wikitext test set for evaluation in the paper)\n",
    "*   Script takes ~5 minutes to run on a T4 GPU instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1T_PnMm7ul5",
   "metadata": {
    "id": "j1T_PnMm7ul5"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_RKvqZ379tED",
   "metadata": {
    "id": "_RKvqZ379tED"
   },
   "source": [
    "## Preliminaries\n",
    "Setting model names and number of documents to evaluate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7befde",
   "metadata": {
    "id": "ec7befde"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "N_TEST = 50\n",
    "DISTIL_NAME = \"distilbert-base-uncased\"\n",
    "BERT_NAME = \"bert-base-uncased\"\n",
    "if N_TEST is None:\n",
    "    TEST_DATASET = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "else:\n",
    "    TEST_DATASET = load_dataset(\n",
    "        \"wikitext\", \"wikitext-2-raw-v1\", split=f\"test[:{N_TEST}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RmgBjhRJ97L9",
   "metadata": {
    "id": "RmgBjhRJ97L9"
   },
   "source": [
    "### Helper function\n",
    "Utility function for correcting layer index after patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Tp-NUIHe96Hq",
   "metadata": {
    "id": "Tp-NUIHe96Hq"
   },
   "outputs": [],
   "source": [
    "def set_layer_idx_recursive(block, idx, attr=\"layer_idx\"):\n",
    "    \"\"\"Set `attr` on `block` and all its submodules.\"\"\"\n",
    "    setattr(block, attr, idx)\n",
    "\n",
    "    def _set(m):\n",
    "        setattr(m, attr, idx)\n",
    "\n",
    "    block.apply(_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rrt36clG-Ioo",
   "metadata": {
    "id": "rrt36clG-Ioo"
   },
   "source": [
    "### Evaluation\n",
    "Functions to get number of parameters and evaluate perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d44afa",
   "metadata": {
    "id": "13d44afa"
   },
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    return {\n",
    "        \"non_embedding\": sum(\n",
    "            p.numel() for name, p in model.named_parameters() if \"embed\" not in name\n",
    "        ),\n",
    "        \"total\": sum(p.numel() for p in model.parameters()),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pseudo_perplexity_distilbert(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    max_length=512,\n",
    "    chunk_size=64,  # mask this many positions per forward pass\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (pseudo_perplexity, token_count_used) for a single string.\n",
    "    Pseudo-PPL = exp( mean_i  [ -log p(x_i | x_{i}) ] )\n",
    "    \"\"\"\n",
    "    SPECIAL_IDS = {\n",
    "        tokenizer.cls_token_id,\n",
    "        tokenizer.sep_token_id,\n",
    "        tokenizer.pad_token_id,\n",
    "    }\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"][0].to(device)\n",
    "    attn = enc[\"attention_mask\"][0].to(device)\n",
    "\n",
    "    # Choose positions to evaluate (exclude special/pad)\n",
    "    positions = [\n",
    "        i\n",
    "        for i, tid in enumerate(input_ids.tolist())\n",
    "        if attn[i].item() == 1 and tid not in SPECIAL_IDS\n",
    "    ]\n",
    "    if not positions:\n",
    "        return float(\"nan\"), 0\n",
    "\n",
    "    total_nll = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Process in chunks to control memory\n",
    "    for start in range(0, len(positions), chunk_size):\n",
    "        chunk = positions[start : start + chunk_size]\n",
    "        bsz = len(chunk)\n",
    "\n",
    "        # Build a batch with one masked position per row\n",
    "        batch_input = input_ids.unsqueeze(0).repeat(bsz, 1)\n",
    "        batch_attn = attn.unsqueeze(0).repeat(bsz, 1)\n",
    "\n",
    "        # Replace each row's chosen token with [MASK]\n",
    "        mask_id = tokenizer.mask_token_id\n",
    "        for row, pos in enumerate(chunk):\n",
    "            batch_input[row, pos] = mask_id\n",
    "\n",
    "        outputs = model(input_ids=batch_input, attention_mask=batch_attn)\n",
    "        logits = outputs.logits  # [bsz, seq_len, vocab]\n",
    "\n",
    "        # For row r, take logits at its masked position\n",
    "        rows = torch.arange(bsz, device=device)\n",
    "        masked_logits = logits[rows, chunk, :]  # [bsz, vocab]\n",
    "        log_probs = masked_logits.log_softmax(dim=-1)\n",
    "\n",
    "        # Gather gold-token log-probs from the *original* tokens\n",
    "        gold = input_ids[chunk].unsqueeze(1)  # [bsz, 1]\n",
    "        nll = -log_probs.gather(dim=1, index=gold).squeeze(1)  # [bsz]\n",
    "\n",
    "        total_nll += nll.sum().item()\n",
    "        total_tokens += bsz\n",
    "\n",
    "    ppl = math.exp(total_nll / max(total_tokens, 1))\n",
    "    return ppl, total_tokens\n",
    "\n",
    "\n",
    "def pseudo_perplexity_corpus(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Global, token-weighted pseudo-PPL over multiple texts:\n",
    "    exp( sum_i nll_i / sum_i tokens_i ).\n",
    "    \"\"\"\n",
    "    total_nll, total_tok = 0.0, 0\n",
    "\n",
    "    test_texts = [t[\"text\"] for t in TEST_DATASET]\n",
    "\n",
    "    for t in tqdm.tqdm(test_texts):\n",
    "        enc = tokenizer(t, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        # Reuse the single-string function but also get its internals\n",
    "        ppl, tok = pseudo_perplexity_distilbert(model, tokenizer, t)\n",
    "        if tok > 0:\n",
    "            # Convert back to summed NLL to combine correctly:\n",
    "            total_nll += math.log(ppl) * tok\n",
    "            total_tok += tok\n",
    "    return math.exp(total_nll / max(total_tok, 1)) if total_tok else float(\"nan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tx-woriW-dKf",
   "metadata": {
    "id": "Tx-woriW-dKf"
   },
   "source": [
    "## Interpolation code\n",
    "Model class for creating interpolated BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IweAKs3L-iZ6",
   "metadata": {
    "id": "IweAKs3L-iZ6"
   },
   "outputs": [],
   "source": [
    "class DistilThenInterpolateForMaskedLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        distil_name=DISTIL_NAME,\n",
    "        base_name=BERT_NAME,\n",
    "        n_layers_removed=2,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # DistilBERT with MLM head (we only use its encoder)\n",
    "        self.distil_mlm = AutoModelForMaskedLM.from_pretrained(distil_name).to(device)\n",
    "        # BERT with MLM head (we use its encoder layers AND its MLM head)\n",
    "        self.bert_mlm = AutoModelForMaskedLM.from_pretrained(base_name).to(device)\n",
    "\n",
    "        self.distil_mlm.distilbert.transformer.layer = (\n",
    "            self.distil_mlm.distilbert.transformer.layer[:-n_layers_removed]\n",
    "        )\n",
    "\n",
    "        # layers used to initialize distilBERT (see https://github.com/huggingface/transformers-research-projects/blob/362a490dc36e91359fe76a7a707dc29e663196b2/distillation/scripts/extract_distilbert.py#L55C9-L55C43)\n",
    "        teacher_layers = [0, 2, 4, 7, 9, 11]\n",
    "        n_layers = teacher_layers[-n_layers_removed]\n",
    "\n",
    "        # Select N BertLayer modules to append\n",
    "        bert_layers = self.bert_mlm.bert.encoder.layer  # ModuleList[BertLayer]\n",
    "        if n_layers <= 0:\n",
    "            selected = []\n",
    "        else:\n",
    "            selected = [copy.deepcopy(l) for l in bert_layers[n_layers:]]\n",
    "        self.append_layers = nn.ModuleList(selected)\n",
    "        for i, block in enumerate(self.append_layers):\n",
    "            set_layer_idx_recursive(\n",
    "                block, i + len(self.distil_mlm.distilbert.transformer.layer)\n",
    "            )\n",
    "        self.distil_mlm.config.n_layers = len(\n",
    "            self.distil_mlm.distilbert.transformer.layer\n",
    "        ) + len(self.append_layers)\n",
    "        self.config = self.distil_mlm.config\n",
    "        self.config.tie_weights_ = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # 1) DistilBERT encoder\n",
    "        distil_out = self.distil_mlm.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        hidden_states = distil_out.last_hidden_state  # [B, T, 768]\n",
    "\n",
    "        # 2) Append selected BERT encoder layers\n",
    "        if len(self.append_layers) > 0:\n",
    "            ext_mask = self.bert_mlm.bert.get_extended_attention_mask(\n",
    "                attention_mask, input_shape=input_ids.shape, device=input_ids.device\n",
    "            )\n",
    "            for layer in self.append_layers:\n",
    "                out = layer(\n",
    "                    hidden_states, attention_mask=ext_mask, output_attentions=False\n",
    "                )\n",
    "                hidden_states = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "        # 3) Use **BERT's MLM head** on the final hidden states\n",
    "        logits = self.bert_mlm.cls(hidden_states)  # [B, T, vocab_size]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=loss, logits=logits, hidden_states=None, attentions=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DoMKdp-J-oBh",
   "metadata": {
    "id": "DoMKdp-J-oBh"
   },
   "source": [
    "## Run evaluation\n",
    "\n",
    "Code to run evaluation on DistilBERT, BERT, and interpolated models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596c734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d596c734",
    "outputId": "995ebf4d-69f6-4422-fddd-fd05041a4485"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_NAME)\n",
    "distilbert = AutoModelForMaskedLM.from_pretrained(DISTIL_NAME).eval().to(device)\n",
    "bert = AutoModelForMaskedLM.from_pretrained(BERT_NAME).eval().to(device)\n",
    "# layers used to initialize distilBERT (see https://github.com/huggingface/transformers-research-projects/blob/362a490dc36e91359fe76a7a707dc29e663196b2/distillation/scripts/extract_distilbert.py#L55C9-L55C43)\n",
    "teacher_layers = [0, 2, 4, 7, 9, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a90397",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52a90397",
    "outputId": "5ad31a97-b0b5-464a-fa9f-9b48b14cd6fa"
   },
   "outputs": [],
   "source": [
    "n_params_lst_bert = []\n",
    "perplexity_dct_bert = {\"zero-shot\": [], \"naive pruned\": []}\n",
    "\n",
    "print(\"Evaluating DistilBERT\")\n",
    "n_params_lst_bert.append(get_n_params(distilbert)[\"total\"] / 1e6)\n",
    "perplexity_dct_bert[\"zero-shot\"].append(\n",
    "    pseudo_perplexity_corpus(distilbert, bert_tokenizer)\n",
    ")\n",
    "\n",
    "print(\"Evaluating Interpolated Models\")\n",
    "for n in range(2, 6):\n",
    "    interp_model = DistilThenInterpolateForMaskedLM(n_layers_removed=n, device=device)\n",
    "    n_params_lst_bert.append(\n",
    "        (\n",
    "            get_n_params(interp_model.distil_mlm)[\"total\"]\n",
    "            + get_n_params(interp_model.append_layers)[\"total\"]\n",
    "        )\n",
    "        / 1e6\n",
    "    )\n",
    "    perplexity_dct_bert[\"zero-shot\"].append(\n",
    "        pseudo_perplexity_corpus(interp_model, bert_tokenizer)\n",
    "    )\n",
    "    del interp_model\n",
    "\n",
    "print(\"Evaluating Naive Pruned Models\")\n",
    "interp_model = copy.deepcopy(bert)\n",
    "for n in range(1, 6):\n",
    "    interp_model.eval()\n",
    "    for i in range(teacher_layers[-n] - 1, teacher_layers[-(n + 1)], -1):\n",
    "        del interp_model.bert.encoder.layer[i]\n",
    "    interp_model.config.num_hidden_layers = len(interp_model.bert.encoder.layer)\n",
    "    interp_model.eval()\n",
    "    perplexity_dct_bert[\"naive pruned\"].append(\n",
    "        pseudo_perplexity_corpus(interp_model, bert_tokenizer)\n",
    "    )\n",
    "\n",
    "perplexity_dct_bert[\"naive pruned\"] = perplexity_dct_bert[\"naive pruned\"][::-1]\n",
    "del interp_model\n",
    "\n",
    "print(\"Evaluating BERT\")\n",
    "n_params_lst_bert.append(get_n_params(bert)[\"total\"] / 1e6)\n",
    "for p in perplexity_dct_bert:\n",
    "    perplexity_dct_bert[p].append(pseudo_perplexity_corpus(bert, bert_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IbVKH6md-9Ey",
   "metadata": {
    "id": "IbVKH6md-9Ey"
   },
   "source": [
    "## Plotting\n",
    "\n",
    "Plot performance of interpolated models compared to naive layer pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040d7af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "b040d7af",
    "outputId": "991843d4-c7f8-423b-ee87-16fe9fae817f"
   },
   "outputs": [],
   "source": [
    "mname = \"BERT\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create plot\n",
    "for metric in perplexity_dct_bert:\n",
    "    metric_lst = perplexity_dct_bert[metric]\n",
    "    plt.plot(\n",
    "        n_params_lst_bert,\n",
    "        metric_lst,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        label=f\"{metric.title()} Models\",\n",
    "    )\n",
    "plt.plot(\n",
    "    n_params_lst_bert[0],\n",
    "    metric_lst[0],\n",
    "    marker=\"o\",\n",
    "    color=\"orange\",\n",
    "    label=f\"Distil{mname}\",\n",
    ")\n",
    "plt.plot(\n",
    "    n_params_lst_bert[-1], metric_lst[-1], marker=\"o\", color=\"red\", label=f\"{mname}\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Parameter count (millions)\")\n",
    "plt.ylabel(\"MLM Pseudo-perplexity\")\n",
    "plt.title(\"MLM Pseudo-perplexity vs. Model Size\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
